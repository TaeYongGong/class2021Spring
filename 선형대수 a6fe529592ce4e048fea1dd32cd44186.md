# 선형대수

# **선형대수와 벡터**

벡터는 정보이자 데이터이다. 

다시말하자면, 벡터는 숫자 자료의 나열이다. 

벡터 ↔ 한쌍의 숫자 

모든 것은 데이터로 되어있다.  아날로그 데이터는 디지털 데이터로 표현 가능하다. 

따라서, 모든 것은 벡터값으로 표현할 수 있다.

벡터 공간, 벡터, 선형 변환, 행렬, 연립 선형 방정식 등을 연구하는 대수학의 한 분야가 바로 선형대수이다. 

Matrix > Vector(가로, 세로 축 중 하나) > Scalar(숫자 하나)

# 행렬

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image1.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image1.jpeg)

행렬은 정의상 사각형 형태로 되어있음

## Column과 Row

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image2.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image2.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image3.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image3.png)

가로는 column이라고 부르고 세로는 row이라고 부른다.

## 행렬의 차원과 공간

행렬의 크기를 얘기할 때 dimension(차원)으로 전환되고 이는 행렬의 공간과 동일하다. 

행렬은 벡터로 나눌 수 있고 벡터 공간는 m*n 의 형태 벡터들이 될 수 있는 모든 값으로 이뤄져 있는 공간(= 차원 / N개의 componant가 그 차 원을 다 채우면 그 차원의 vector space 하나를 이룸.)

**whole space** 

벡터 자체가 가 지고 있는 차원 전체

**column/row space** 

column 혹은 row가 채울 수 있는 차원 전체

이것이 바로 column 측면에서 는 m차원 row 측면에서는 n차원이 된다.

 하나의 행렬은 m이라는 차원과 n이라는 차원을 whole space로 가진다.

**null space** 

 whole space에서 어떤 column/row space를 정의하고 나서 그 나머지(기하학적 정의)

어떤 행렬이 있을 때 무엇을 곱하든 0으로 이루어진 행렬이 될 때, 이 모든 값들이 이루는 공간

cf) column서는 왼쪽에다 곱해야지 차원이 같아지기 때문에 left null space라고 한다. Row의 입장 에서는 오른쪽에서 곱해야지 차원이 같아지기 때문에 null space라고 한다. 그리고 하나의 행렬에 서 column입장에서 null space를 빼고 row입장에서 null space를 빼면 둘다 같은 차원을 가진다. 즉, whole space는 달라도 둘의 차원은 같다는 뜻이다.

## 행렬과 벡터

이 하나의 column 혹은 row를 벡터로 표현가능

### 기하학적 정의

벡터는 벡터의 차원 공간에 한 점으로 나타낼 수 있음. 

벡터는 공간상의 한 화살표이다. 

길이와 방향을 가지고 있고 차원을 가지고 있음.

2차원이라는 말은 숫자가 두줄로 배열됐음을 의미한다. 

백터의 좌표는 한쌍의 숫자

좌표상에 표현하면  머리가 꼬리(원점)으로 부터 얼마나 떨어져 있는냐를 나타냄. 

(굳이 원점에서 시작할 필요는 없지만 편의상 원점에서 부터 시작한다고 가정함.)

$\begin{bmatrix} x \\ y\end{bmatrix}$에서 x는 x축으로부터 얼마나, y는 y축으로 부터 얼마나 떨어져 있는 지를 나타냄.

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled.png)

2차원, 2차원 상의 한점

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%201.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%201.png)

3차원, 3차원 상의 한점

벡터는 크기와 방향성만 중요함

크기와 방향성만 정해지면 어디로 가든지 상관이 없음. (원점에서 시작할 필요가 없음)

# 벡터의 계산

**상수곱**

상수와의 곱셈은 벡터의 길이를 변형시킴. 이를 scaling이라고도 함. 

벡터를 scaling하는 상수를 scalar라고 부른다. 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%202.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%202.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%203.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%203.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%204.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%204.png)

벡터에 곱해진 값(상수)은 벡터의 길이에만 영향을 주 크기에 영향을 주지 않음.

상수값을 통해 확장하면 vector space을 이루게 된다.

음의 상수를 곱하면 벡터가 뒤집힌 상태가 된다. 

**벡터의 덧셈**

두 벡터의 덧셈은 (평행사변형 형태로) 새로운 방향으로 바꿔줌. 

다른 말로 하자면 두 벡터 중 하나의 시작을 원점이 아닌 다른 벡터의 머리에서 시작하게 이동시키고 도출된 머리가 두 벡터를 더한 꼬리가 원점인 새로운 벡터의 머리가 된다

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%205.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%205.png)

vector space을 확장하지는 않는다. 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%206.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%206.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%207.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%207.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%208.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%208.png)

## **벡터 product**

product의 종류는 세 가지

- inner product 내적
- outer product 외적
- tensor product 텐서곱

### inner product 내적

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%209.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%209.png)

### **기하학적 정의**

 $|a|\cos{\theta}$에서 $|a|$는 벡터 a의 크기를 나타내며 이에 $\cos{\theta}$을 곱하는 것은 벡터 a의 방향을 b의 방향으로 변형시키는 것이고(이를 다른말로 **project on to**이라고 한다.) $|a||b|\cos{\theta}$은 결국  벡터 a와 크기가 동일하면서 벡터 b와 동일한 방향인 새로운 벡터를 벡터 b에 곱하는 것과 동일하다. 

반대도 경우도 동일한 값이 도출된다. 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2010.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2010.png)

결과적으로 두개의 2d input을 통해 하나의 1d output이 도출된다. 

기저벡터의 차원에서 설명한다면 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2011.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2011.png)

두 벡터 중 하나를 기저벡터 분해한뒤 이를 기저벡터로 갖는 동일차원의 공간에서 남은 벡터의 크기를 계산한 결과가 inner product인 것이다. 

위의 계산과 반대로 해도 대칭이기 때문에 동일한 결과값이 도출된다. 

### **대수적 정의**

$$\begin{bmatrix} a \\ c \\e \\.\\.\\.\end{bmatrix} \cdot \begin{bmatrix} b \\ d \\ f \\.\\.\\.\end{bmatrix} = a*b + c*d + e*f + ....$$

matrix-vector product와 dot product은 동일하다. 

matrix-vector product

$$\begin{bmatrix} nx & ny \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix} =nx*x + ny*y$$

dot product

$$\begin{bmatrix} nx \\ ny \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix} =nx*x + ny*y$$

행렬로 치면 하나의 행렬을 tilt해서 곱하는 것과 동일하다.

### Informational Similarity

이를 구하는 이유는 두 벡터(두 정보의 나열)간의 informational similarity를 구하기 위해서이다. 

1. $\cos{\theta}$를 통해(=cosine similarity)
    - $\cos{\theta}$는 두 정보 나열간 얼마나 떨어져 있는 지를 나타내는 지표이다.
    - $|a|와 |b|를 \ 알고 \ 있고 \ 대수적 \ 정의를 통해 \ inner \ product\ 값을 \ 도출할 \ 수 \ 있기 \ 때문에$  우리는 $\cos{\theta}$의 값을 도출할 수 있게 된다.

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2012.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2012.png)

    - $\cos(0) = 1\\ \cos(\pi / 2) = 0$  = orthogonal 두 벡터간 관련이 없다.

2. $a-b \ distance$을 통해 

- 거리를 구하는 공식: **유클리디안 공식**

    두 개의 벡터 각각 x component와 y component을 가지고 있으며 이 둘간의 거리는 좌표축 상에서 

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2013.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2013.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2014.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2014.png)

로 구할수 있다. 

다차원의 경우 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2015.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2015.png)

선형변환 : 변환의 적용은 벡터의 내적을 구하는 것과 같다. 

두개의 벡터값은 결합이 가능한데 이를 linear combination이라고 하고 이는 선형성(같은 직선위 에 존재하는 여부 / 독립성 여부)를 판단하는 기준이 된다.

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image4.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image4.jpeg)

# correlation and vector

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2016.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2016.png)

### 기본개념

상관관계(correlation) : 같이 가는 느낌 -> correlation r의 값 : -1<=r=<1

- 1와 1의 값이 나올려면 선상에 있으면 된다. -> 얼마나 선상에 가까우냐에 따라 –1,1 / 상관 이 없을 때 -> r의 값이 0
- 0보다 작으면 negative corr, 1보다 크면 positive corr

correlation과 cosine similarity와 직접적으로 연관되어 있다. 

> 벡터의 내적 (inner product)과 상관계수

- 여러 개의 수치가 있어도 두 변수만 있다면 이 모든 수치는 두 개의 벡터로 표현할 수 있고 이 두 벡터는 차원이 넓다고 해도 삼각형 형태를 벗어 나지는 않는다.
- cos을 적용 가능
    - ex) $a = [1,2,3], b = [2,4,7]$

        T -> 행과 열을 바꿔 준다., 행렬식의 연산을 보여주기 위해(dot product을 적용하는 것과  동일) 

        $a*b^T$ 

        $a⋅b(∑ai*bi)$로도 표현가능 

        cf) $a^T*b$  행렬로 나오게 하는 방법 -> outer product

        b벡터의 선(길이)에 project된 a벡터의 값(길이)를 곱하면 내적 값이 나온다. 이를 알려면 $a⋅b = |a|*\cos(\theta)*|b|$를 구하면 된다.

    결국  $a⋅b(inner\ product)/|a||b|=cos(θ)=r($상관계수식으로 증명) -> 두 변수의 공분산 -> 벡터의 내적 / 두 변수 각각의 표준편차의 곱-> 각각 벡터 길이의 곱 $−1≤ cos(θ) = r ≤1(cos\ similarity)$\ -> 두 개의 벡터를 주고 이를 구해라 -> 얼마나 비슷한지를 구하는 법

### 통계학적 공분산과 상관계수

> 공분산(Covariance)

두 확률변수 X와 Y가 어떤 모양으로 퍼져있는지

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image14.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image14.png)

즉, X가 커지면 Y도 커지거나 혹은 작아지거나 아니면 별 상관 없거나 등을 나타내어 주는 것 이다.

- Cov(X, Y) > 0
    - X가 증가 할 때 Y도 증가한다.
- Cov(X, Y) < 0
    - X가 증가 할 때 Y는 감소한다.
- Cov(X, Y) = 0
    - 공분산이 0이라면 두 변수간에는 아무런 선형관계가 없으며 두 변수는 서로 독립적인 관계에 있음을 알 수 있다.

그러나 두 변수가 독립적이라면 공분산은 0이 되지만, 공분산이 0이라고 해서 항상 독립적이 라고 할 수 없다.

어떻게 하면 그것을 나타낼 수 있을까 고민한 결과 공분산은 아래와 같이 구하기로 하였다.

- 확률변수 X의 평균(기대값), Y의 평균을 각각(설정) 이라 했을 때, X,Y의 공분산은 아래와 같다.

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image15.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image15.jpeg)

- 공분산 : X의 편차와 Y의 편차를 곱한것의 평균이라는 뜻

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image16.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image16.jpeg)

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image17.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image17.jpeg)

    ![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image21.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image21.jpeg)

- X와 Y가 독립이면이므로 공분산은 0
    - **공분산에도 문제점이 하나** 있음. **.X와 Y의 단위의 크기에 영향을 받는다는** 것이다.

        즉 다시말해 100점만점인 두과목의 점수 공분산은 별로 상관성이 부족하지만 100점만점이기 때문에 큰 값이 나오고 10점짜리 두과목의 점수 공분산은 상관성이 아주 높을지만 10점 만점이기 때문에 작은값이 나온다.

> **상관계수**

- 이것을 보완하기 위해 **상관계수(Correlation)가 나타난다**. 상관계수라는 개념이 왜 나왔는지 생각하다 보면 의외로 간단하다. **확률변수의 절대적 크기에 영향을 받지 않도록 단위화 시켰다고 생각하면 된다**. 즉, **분산의 크기만큼 나누었다고 생각하면 된다.**

**상관계수의 공식**

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image22.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image22.jpeg)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image23.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image23.jpeg)

**상관계수의 성질**

1. **상관계수의 절대값은 1을 넘을 수 없다.**
2. 확률변수 X, Y가 독립이라면 상관계수는 0이다.
3. **X와 Y가 선형적 관계라면 상관계수는 1 혹은 –1이다.**

> 상관관계에서 선형대수가 쓰이는 방법

차원이 높아져도 삼각형을 이룬다. 여러 개의 벡터값이 직선을 이룬다.

선형결합 vector의 개수는 변수의 개수지 변수 값의 개수가 아니게 된다.

**하나의 변수는 하나의 vector로 모든 변수값을 나타낼 수 있다.** 

**각도 값(θ)에 cos(θ)을 해주면 r 값이 된다.**

**관련성이 없을수록 각도값이 커진다(최대 90도) -> cor값은 0**

**관련성이 있을수록 각도값이 작아진다.(최소 0도) -. cor값은 1** 

 **90도 초과해서는 –1에 가까 워 진다.**

> 어떤 두 벡터가 있을 때 A라는 벡터 B라는 벡터가 있을 때 그 사이의 각도값을 구하는 방법

 inner product : 안쪽으로 다 곱해서 더한 값

ex) (1 2 3) a 벡터 (둘이 하나의 행렬을 이룸)

(4 5 6) b벡터 => inner product 값 : 32

**= 하나를 수직으로 내려서 두 길이를 곱한 값 (기하학적) a벡터의 길이 * cos(θ) * b벡터의 길이 = inner product**

> **벡터의 길이 구하는 방법**

길이 구하는 방법 

if $A = [a \ b \ c \ d \ ....]$

$A의 \ 길이 = \sqrt[]{a^2 + b^2 + c^2 + ....}$ 

why? 

1차원 → 주어진 벡터의 길이는 주어진 좌표

2차원 → 주어진 벡터의 길이는 피타고라스의 정리를 이용하여 구할 수 있음

3차원 → 주어진 벡터의 길이는 직육면체상의 대각선길이(피타고라스의 정리 응용)

위와 같은 방식으로 한 차원 한 차원 장인 정신으로 직선을 압축시켜 가면 n차원 까지 벡터의 길이를 구할 수 있고 그것을 공식화 하면 n차원의 벡터 v의 길이는 

 $|v| =  \sqrt[]{{v_0}^2 + {v_1}^2+ {v_3}^2+ ....+{v_n}^2}$ 

> **벡터와 상관계수**

상관계수 공식에서

각 벡터의 하나의 scalar를 각각

$ai=xi−\bar{X} \\ bi=yi−\bar{Y}$라고 하면 

각각의 벡터는  

 $\vec{a}=∑xi−\bar{X}$(x의 표본평균)

 $\vec{b}=∑yi−\bar{Y}$(Y의 표본평균)라고 표현할 수 있고

위의 길이 공식에 따라 각 벡터의 길이는 각각

$$|\vec{a}| = \sqrt[]{{x_1} ^2 + .... + {x_n} ^2 } \\ |\vec{b}| = \sqrt[]{{y_1} ^2 + .... + {y_n} ^2 } $$

이된다. 

따라서,  시그마 공식에 의해 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image24.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image24.jpeg)

$$\frac{ \vec{a} \cdot \vec{b}(= inner \ product = \ cov(X,Y))}{|\vec{a}|*|\vec{b}| (=\sqrt{var(X)*var(Y)})}=cos(θ)=r \\ -1\le \cos(\theta) \le 1$$

을 유도할 수 있다. 

**상관계수와 공분산 모두 공통적으로 벡터의 내적을 이용해 설명할 수 있고, 데이터 간의 (즉, 벡터 사이의) ‘닮음’과 연결 지을 수 있다.**

[상관 계수는 벡터의 내적이다](https://www.youtube.com/watch?v=kzqTTCB-Luo)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2017.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2017.png)

따라서, 위의 식의 similarity를 구해보면 

$$|a| = \sqrt[]{90^2 + 48^2 + 60^2 + ....} \\ |b| = \sqrt[]{92^2 + 54^2 + 55^2 + ....} \\ \cos{\theta} = \frac{a \cdot b(=dot\ product)}{|a||b|} = r(=correlation)$$

이 된다. 

# 소리와 상관계수

### 기본개념

소리도 데이터이기 때문에 벡터 값으로 표현 가능하다. 

소리 데이터를 이산적으로 sampling한다면 여러개의 점으로 표현가능하고 이는 cos그래프로 표현이 된다. 

이 벡터에다가 사인 웨이브를 여러개 만들어서 각각 inner product을 해 버리면(|→a||→b|값은 고정 / 소리의 벡터 값도 고정되어있기 때문 / 지속시간이 존재 ) 어떤 값이 나올 텐데 데 서로서로 유사한 성분이 많이 있으면(correlation이 높으면) inner product 값이 높게 나올 것이다.

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2018.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2018.png)

위의 경우에는 두 벡터의 dot product은  $a \cdot b = |a| * \cos(0) * |b|$가 되며 cos(0)은 1이기 때문에 동일한 sound wave라는 것을 알 수 있다. 

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2019.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2019.png)

위의 경우에는 두 벡터의 dot product은  $a \cdot b = 0 = |a| * \cos(90) * |b|$가 되며 cos(90)은 0이기 때문에 correlation이 0인 두 sound wave라는 것을 알 수 있다. 

### wave와 벡터

> wave가 있다고 생각하면 그냥 눈으로 볼 때 여기에 어떤 성분들이 2개가 보이는가?

(정확한 이해를 위해서는 푸리에 변환에 대한 이해가 필요하다. )

[그려보는 수학 | 푸리에 변환 -- 1. 관점의 변환 : 시간 vs. 주파수](https://www.youtube.com/watch?v=60cgbKX0fmE)

[페이저(phasor)](https://angeloyeo.github.io/2019/06/18/phasor.html)

**sin wave**

1) 주파수

- 벡터값으로 표현 가능두 개의 주파수의 inner product를 이용 가능
- sin wave를 만드는 것은 simple phasor 그리고 sin wave은 cos wave로 표현가능하다.
    - (phasor 분석에서 중요한 전제는 모든 sin wabe를 cosine으로 환원해서 생각하는 것으로부터 출발하자는 것이다. 그래야만 위상의 기준점을 설정할 수 있기 때문이다. 다시 말하자면, sine은 막대기가 π/2 만큼 이동한 곳에서 막대기가 회전을 시작함. )
    - 만든 만큼 곱하면 그 숫자만큼 만들어짐
    - 그 phasor를 이용하여 cos similarity를 이용하면 어떤 부분과 같은지가 구할 수 있다.
        - 어떤 성분 (sin wave)가 많은지를 구해서 이를 종합하면 된다.
            - ex) a라는 사인 웨이브, b라는 사인 웨이브 (a랑 비슷),  c라는 사인 웨이브(a,b보다 더 빠름)
            - aㆍb > aㆍc -> 같은 성분에만 response
                - 같은 시간 상에서 a sin wave에서는 양수값/ c sin wave에서는 음수값 인경우가 많기 때문

 2) amplitude

b sin wave에서는 양 -> 양 / 음 -> 음 인 부분이 많음 -> 내적 값이 더 높게 나올 수 밖에 없음

어떤 웨이브가 있다고 치면 값이 같으면 내적 값이 높게 나오고 값이 다르면 내적 값이 낮 게 나온다. 내적값이 높다 = correlation이 높다

projetion의 길이는 cos/ angle 값에 의하여 결정된다 .

0(최대)에 가까울수록 커지고 π/2(최소)에 가까울수록 0에 가까워진다.

> 내적값으로 보는 correlation의 오류

시작 점이 다른 wave(주파수<속도>는 same)하나는 sin wave 

하나는 cos wave (차이 90도 / 1/2π)-> 둘의 내적 값은 ‘0‘이 된다.

a벡터와 b벡터가 1/2π가 될 때 -> 두 벡터의 내적 값은 0이 된다. 

(9차원 상의 벡터) phasor의 이동에 따라 민감하게 반응 한다.

왜냐하면 cos(1/2π)은 0인데 →a⋅→b=|→a|*cos(θ)*|→b|이 공식을 통해 보자면 내적의 값 (분자)가 0일 때 0의 값이 될 수 있기 때문이다.

> 오류의 해결

민감하게 반응하는 정도를 줄이려면  complex phasor을 써서 inner product를 해야함.

이러면 phasor에 대한 민감도를 줄일 수 있음 즉, 2차원이 아닌 3차원과의 결합을 통해 오류를 줄일 것임.

### spectral analysis

어떤 소리든지 wave(무수한 주파수의 합)가 있을 때 어떤 주파수 성분이 많은지 알고 싶을 때 어떤 wave(complex wave)에는 무조건 그 성분(주파수)이 들어 있다. 여기서 중요한 점은 얼 마나 많은지다.

ex) 

어떤 주파수는 0.001, 어떤 주파수는 10 이렇게 들어 있는데 이를 분석하는 것이 spectral analysis  

여기서 쓰는 것이 inner product다. 

주파수를 벡터값으로 바꿔서 그 wave와 inner product를 해서 그 값이 클수록 그 성분이 많다. 

 wave속의 성분(주파수)의 개수라고 생각해도 된다.

여기서 주의해야 하는게 pahsor는 두가지 종류 simple phasor와 complex phasor

$complex \ phasor = \cos(θ) + e^{sin(θ)i}$

simple phasor의 경우에는 오류가 발생할 수 있다. 

만약, 시작도 똑같은 sin, cos 그래프가 있을 때 똑같은 주파수를 가지고 있는데 inner product 값은 0이 된다. cos(π/2) = 내적 / 두 벡터 길이의 곱 = 0 

내적 값(분자)은 0 target이 이동함에 따라 나오는 값이 달라짐.

3차원으로 분석한 complex wave로 내적을 하면 위의 현상이 없어짐. complex wave 

허수 포함 -> cos, sin, i(허수)의 3차원

이 3차원의 complex wave와 내적을 하면 하나의 값이 나온다.

중간에 들어가는게 complex number이기 때문에 값은 complex number가 된다. 

허수를 포함하고 있다. 

->그러면 ploting이 불가능 할 수 있는데 

$$a+bi(complex \ number)$$

를 하면 실수가 나온다. (허수는 cos값으로 나올 수가 없음) -> 이러면 0으로 나올 수가 없다.

cf) complex number의 절댓값을 구하는 방법 

실수 축(x)과 허수 축(y)의 (a,b)를 찍을 수 있는데 이때의 절댓값은 원점으로부터 (a,b)의 길이다.

ex) sample이 30개다 하면 30개의 complex wave를 만들어 준다. 각 주파수 별 energy가 얼마나 되는 지를 알 수 있다.세 개의 그래프간의 관계 이해

1. cos 그래프
2. **correlation 그래프**
3. 벡터 그래프 사이의 관계를 이해해야 한다.

r = -1일 때, 음의 관계를 가질 때 -> 벡터 사이의 각도는 180도이다./ 방향성이 반대이다. sin 그래프를 180도 shift하면 올라갈 때 내려갈 때의 원래 sin graph의 정 반대값을 가진다.

= 180도 위상 차이가 있다. = 이는 음의 관계만 가지면 되지 amplitude와는 상관이 없다. 

(amplitude는 벡터의 길이와만 상관있지 벡터간의 관계에는 상관이 없다.)

---

# 선형대수 심화 이해

## outer product(cross product)

하나의 삼각형이 만 들어지고 이 삼각형이 무수히 확장되면 column vector space를 만들 수 있다.

이를 넓이의 차원에서 본다면a라는 벡터가 하나있고 b라는 벡터가 하나있는데 이것의 내적은 이들이 평행사변형의 넓이와 동일하다. 

이 때 형성된 평행사변형의 넓이는 두 벡터의 곱의 값과 동일하다 

벡터 크기는 차원을 늘릴 뿐 점을 늘리지는 않는다. 반대로,  두 벡터의 곱셈(product)은 벡터 값의 차원을 틀어준다.(변형시킴) 

## Linear combination

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image5.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image5.jpeg)

이 두개는 independent하다고 한다. -> 같은 선상에 없기 때문.

그래서  col1과 col2의 linear combination으로 다른 점을 나타낼 수 있다.

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image6.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image6.png)

이렇게 한 직선상에 있으면 dependent하다고 한다.

삼각형이 만들어 지지 않는다. -> 여기서 확장을 해도 line이 확장될 뿐 line 안쪽을 채울 수 없음

이를 독립성을 행렬식으로 표현하면

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image7.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image7.png)

## Column space

- 독립적이 아니다.
- linear combination으로 만들 수 있으면 이는 independent 하다고 할 수 없음.
    - 1,2,4 / 1,1,1 이 두개에 의해 2,3,5를 만들 수 있기 때문에 2,3,5는 존재하지 않는다고 할 수 있음(만들어 지는 것이지 새로운 값이 아니다.)
    - 결국 whole space는 3차원이지만 column space는 2차원이 된다.

## Transpose

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image8.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image8.png)

T는 transpose했다는 뜻이다. 

 3*2 -> 2*3로 바꿨다 -> whole space는 2차원 <여기서 whole space는 열의 관점에서 봤을때의 whole space / 2차원 값이 벡터가 3개 있는 것>

Column vector는 3개가 있는데 만들 수 있는 것은 2차원이기 때문에 column space는 2차원이다. Transpose를 해도 여전히 whole과 vector의 개수는 달라져도 column space는 2차원

## Matrix Space 추가 정리

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image9.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image9.jpeg)

m*n 행렬이 있다고 생각할 때 m 크기의 whole space<column>가 하나 있고 

n 크기의 whole space<row>가 있다. 

M이랑 같거나 작게 column space<column>가 있고 

N이랑 같거나 작게 column space<row>가 또 존재 한다. 

여기서 column space의 차원을 판단하는 기준이 독립적이냐 독립적이지 않느냐에 따라 판단한다. 여기서의 나머지를 left null space / null space라고 한다.

## Vector와 Matrix의 계산 추가 정리

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image10.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image10.png)

X가 입력 벡터, b가 출력 벡터, A는 행렬

X의 벡터 차원과 b라는 출력 벡터의 차원은 같지 않아도 된다. A 행렬이 그 역할을 해준다.

A라는 행렬은 x의 벡터 크기에 따라 크기가 정해지고

B라는 벡터는 A라는 행렬에 따라 크기가 정해진다.

A라는 행렬로 b라는 벡터가 달라지기 때문에, A를 linear transformation이라고 한다.

<차원도 바꾸고 값도 바꾼다.>

이를 그래프 상으로 보면 basis vector의 차원을 틀어주는 것과 같다.

### 기하힉적 해석

선형변환은 공간을 이동시키는 방법이며 격자선이 여전히 평행하고 균등간격을 유지한 변형이다. 그리고 원점은 고정되있음을 의미하고 이 변환들을 간단한 숫자들로 설명가능 하다. 바로 기저 벡터들는 변환후의 좌표값이다.

행렬은 우리에게 이러한 변환을 설명하는 언어를 제공해줌.

행렬의 열들은 이 좌표 값을 나타내며, 행렬=벡터 곱셈은 단지 이것을 계산하는 방법임. 이 변환이 주어진 벡터에 적용한 결과를 보여줌.

우리가 행렬을 볼때마다 공간의 어떤 변환으로 생각가능.

### 역함수의 형태

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image11.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image11.png)

출력에 해당되는 부분에 A의 역행렬을 집어넣어서 입력 값을 유도하는 과정 하지만, 만약에 A로 인하며 b가 일직선이 되면

여기 있는 점을 역으로 했을 때 이 점은 원래 자리로 찾아 갈 수 없음

직선에서 펼치려고 하면 어디서 왔는지, 어디로 가는 지 알 수 없음 -> 이를 invertible하다고 한다.

행렬을 배운 사람 determinant<[선형대수학](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99)에서, **행렬식**은 [정사각행렬](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%82%AC%EA%B0%81%ED%96%89%EB%A0%AC)에 수를 대응시키는 [함수](https://ko.wikipedia.org/wiki/%ED%95%A8%EC%88%98)의 하나이다. 대략, 정사각행렬이 나타내는 [선형 변환](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EB%B3%80%ED%99%98)이 부피를 확대시키는 정도를 나타낸다.> 이 값 이 0이라고 할 때 역행렬이 존재하지 않음

면적이 0일 된다면(독립적이면) -> 0이 됨과 동일해지고 왜 inverse가 존재하지 않느냐를 설명해준 다.

## Null space

Ax=0의 모든 해 집합을 행렬 A의 **영공간(null space)**이라고 합니다. A가 n개 열을 가졌다면 A의 영공간은 Rn의 부분공간이 됩니다.

> 영공간을 선형변환(linear transformation) 관점에서 이해할 수도 있습니다. T를 n차원 벡터 x를 m 차원 영벡터로 변환하는 선형변환으로 둔다면 영공간 NulA는 아래 그림처럼 도식화할 수 있습니 다.]

M*N인 행렬이 있을때

M은 column의 관점 / column space

M차원이 column의 whole space

만약에 column space가 whole space를 다 채우지 못한다면 이는 null space라고 한다.

<Null space는 그 벡터 값에 곱하면 0을 만들어주는 행렬>

또한 모든 column이 독립적이면 (한 일직선상에 존재하는 행렬이 하나식만 존재) column space는 whole space다.

여기서 독립적인 행렬은 whole space가 m차원이면 최대 m개까지만 존재가능하다.

 m+1차원 행렬은 m차원의 행렬의 조합으로 표현가능하다.

만약에 column vector가 두개가 있다면 행렬의 덧셈으로 그 vector 값을 구할 수 있다. 이는 곧 두 vector에서의 평행사변형 꼴의 한 점이 된다.

Vector가 한 직선상으로 되어있지 않으면 두개의 rank으로 되어있는 행렬이다.

N은 row의 관점 -> N차원이 row의 whole space

2차원 row가 3개가 있다고 하면 2개의 조합으로 나머지 하나를 만들 수 있기 때문에 3개 전부

independent할 수 없다. 결국, n차원에서 independent한 벡터는 총 n개만 존재한다. 요약)

R^m –> m이 column의 whole space-> column space R^n -> n이 row의 whole space -> row space

그리고 둘의 차원은 같아야 한다. (whole space는 달라도)

V는 벡터 W도 벡터(차원이 같음) / c,d는 곱하는 숫자

null space가 필요한 이유 2*3 3*1 = 2*1

whole space는 3차원

row space는 2차원 (서로서로 independent 한 상황 -> 2차원) 두 개의 row vector

여기서 null space는 1차원(=row space의 평면을 직각으로 지나가는 직선 하나, 평행하게 따 라가는 직선 / 이 방향으로의 변환은 출력에 영향을 미치지 않는다.)

출력에 영향을 미치는 공간은 row space / 영향을 미치지 않는 공간 null space

### 기하학적 해석

위에 설명했던 행렬로 설명을 하면, Colum space가 있는 전체 집합은 3차원이다.

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image12.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image12.jpeg)

Row space는 row들이 span 하는 subspace를 의미하며, 기본적으로 2차원에 포함된다.

선형 독립하는 벡터를 Gram Schmidt를 통해서 각도가 1도라도 삐져나온다면, projection을 하여 수직은 벡터를 찾을 수 있다. 즉, 선형 독립(linearly independent)하는 벡터를 찾을 수 있다면 Gram Schmidt를 하여 직교하는 벡터를 찾을 수 있다.

> 각도가 선형 종속(linearly dependent)이라면 평면 안에 들어온다는 뜻이다. Projection 하면 평면과 벡터의 각도는 '0'이 된다. 1도만 있어도 선형 독립(linearly independent)하게 된다. 직교 (orthogonal)한다는 그것은 이 각도가 90도가 된다. 따라서, 선형 독립이 직교하지 않을 수 있지만 직교한 벡터는 항상 선형 독립이다.‘

Row A 벡터 [1, 2, 3]과 선형 독립인 벡터를 직교하는 벡터들의 집합은 평면을 이루므 로, Row space가 선이라면, Null space는 평면이 된다. 따라서 Null A 또는 base 벡터는 2 차원이 된다….

1. **수학적 해석**

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image13.jpeg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/image13.jpeg)

Ax=0이라고 정의 (null space)

given A에 대해서 0(영행렬)이 되는 x값을 찾아라 -> x값의 공간이 null space

> 내적 관점에서는 '0'을 만족하는 x, y를 구하는 것이나, 서로 직교한다면 내적이 '0'이 되는 개념을 활용한다면, 매트릭스  의 각각의 row 벡터(a1, a2, A3)와 직교하는 [x, y] 벡터의 좌표를 구하는 것과 같다.

### 응용의 해석

결국, null spaces는 어떤 입력이 들어오든 출력에 영향을 미치지 않은 공간 -> 입력이 변하 더라도 출력에 영향을 미치지 않은 공간을 줄이면서 필요 없는 정보를 점점 줄일 수 있음. -> 변환해서 최대한 간단히 해서 줄이는 것이 중요

그래서, null space와 그렇지 않은 공간을 나누는 것이 핵심 / 방해받는 공간을 피할 수 있게 해줌 = 입력이 많이 변해도 -> 출력의 영향 여부로 판단 -> 하지만 수학적 해석처럼 꼭 출력 값이 0일 필요는 없음

$E = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ -> 어떤 값을 집어넣어도 그대로 되는 행렬, 단위 행렬이라고도 한다. 

## eigenvector(고유벡터)와 eigenvalue(고유값)

어떤 행렬의 eigenvector란 무엇이냐?

어떤 벡터는 행렬 A와 transformation을 했음에도 불구하고 원점과의 일직선상에 있는 경우가 존 재하는데 이 벡터를 eigenvector라고 한다.

행렬 A를 선형변환으로 봤을 때, 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)라 한다.

### 기본개념

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2020.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2020.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2021.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2021.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2022.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2022.png)

basis vector라고도 한다. 이 상태에서 grid를 바꿔서 값을 변경 가능

V는 원래 벡터값 -> 입력 / Av는 변환된 벡터값 -> 출력 / 변환된 값은 평행사변형의 꼭짓점 으로 바뀐다. 이게 기본적인 transformation

> Av = λv // (A − λI) v = 0 (v -> 0을 포함하면 안 된다.)

> det(A − λI) = 0

v는 고유벡터 <eigenvector> / λ는 고유값(상수) <eigenvalue>이 된다. -> eigenvector의 방 향성은 바뀌지 않고 크기만 바뀐다(vector 값의 확장이 된다) -> 모든 벡터값은 한 직선상에 있다.

- 결국 고유벡터를 통해서 입력벡터을 만들어 주어 계산을 간소화한다. -> 행렬에서의 계산 에서 상수로의 계산으로 대체되기 때문에 계산과정이 간소화된다..

원점과 입출력이 평행한 순간이 존재 -> eigenvector

> eigenvalue -> 벡터값이 변하는 정도 / 크기 (= 직선일 때의 AV/V의 값이 eigenvalue이다)

2차원에서는 2개가 존재(eigenvector가 두 개 존재하기 때문)이를 수식으로 표현하면 Av=λv.

v는 고유벡터 / λ는 고유값이 된다.

- > 결국, 입력 vector 값에서 방향성은 바뀌지 않고 크기만 바뀐다.
- > 행렬에서의 계산에서 상수로의 계산으로 대체되기 때문에 계산과정이 간소화된다..

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2023.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2023.png)

### 고유값 분해

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2024.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2024.png)

![%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2025.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%83%E1%85%A2%E1%84%89%E1%85%AE%20a6fe529592ce4e048fea1dd32cd44186/Untitled%2025.png)

> To begin, let v be a vector (shown as a point) and A be a matrix with columns a1 and a2 (shown as arrows). If we multiply v by A, then A sends v to a new vector Av. If you can draw a line through the three points (0,0), v and Av, then Av is just v multiplied by a number λ; that is, Av=λv. In this case, we call λ an eigenvalue and v an eigenvector.Those lines are eigenspaces, and each has an associated eigenvalue. Second, if you place v on an eigenspace (either s1 or s2、대칭하여 2개가 생김 양의 방향과 음의 방향) with associated eigenvalue λ<1, then Av is closer to (0,0) than v; but when λ>1, it'sfarther. Third, both eigenspaces depend on both columns of A: it is not as though a1 only affects s1.

### null space and eigenvector

벡터라는 것은 matrix의 한 형태라고 했지만, 물리학적인 개념으로 벡터는 방향이다.

null space도 방향이 중요 / eigenvector도 방향이 중요

> eigenvector -> 어떤 행렬이 존재할 때 이 행렬의 eignvector는 무엇인가? 이러한 질문을 해 야 한다. 원점에서의 직선 -> 이것의 방향 -> eigenvector는 2*2에서 두 개가 있다. 3*3에서 는 3개가 있다.

**eigenvector는 왜 배우는가?**

> column vector가 두 개 -> 이를 또 다른 두 개의 vector로 바꾼 것 -> eigenvector가 2개 존재하기 때문 -> 하는 이유? 훨씬 더 본질적/ 고유한게 무엇인지 판단하게 해줌. 2개 중에 어떤 게 더 고유한가? -> 행렬 대신 λ로 계산 가능하게 해준다. -> 즉 계산을 빠르게 하기 위해, / 어떤 벡터든지 고유벡터(eigenvector) <Av = λv가 되게 하는 v>들의 선형결합으로 표현을 하고/ 각각 벡터와 그 값을 곱한 걸 나중에 합치는 과정(=eigen decomposition)을 살 펴보고자 한다. 통계학에서 PCA / 분산분석을 하게 해준다.